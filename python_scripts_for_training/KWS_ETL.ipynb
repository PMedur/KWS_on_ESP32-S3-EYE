{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c83f435-07ca-478b-a5d8-c998bfb0c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import python_speech_features\n",
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb101e5-aa6c-4e31-8c0d-eab49acb6520",
   "metadata": {},
   "source": [
    "## Audio Feature Extraction Functions\n",
    "\n",
    "Utility functions for processing audio files and extracting MFCC (Mel-Frequency Cepstral Coefficients) features.\n",
    "\n",
    "### `signal_to_mfcc()`\n",
    "Converts raw audio signal to MFCC features using specified parameters:\n",
    "- **Window length:** 25ms\n",
    "- **Window step:** 20ms\n",
    "- **Number of coefficients:** 13\n",
    "- **Frequency range:** 300-4000 Hz (optimized for speech)\n",
    "- **Returns:** Transposed MFCC matrix (features × time)\n",
    "\n",
    "### `calc_mfcc()`\n",
    "Loads audio file and extracts MFCC features:\n",
    "- Loads audio at 8kHz sample rate using librosa\n",
    "- Applies `signal_to_mfcc()` for feature extraction\n",
    "\n",
    "### `chunk_audio_signal()`\n",
    "Splits long audio into fixed-duration chunks (default: 1 second):\n",
    "- Prevents variable-length inputs\n",
    "- Each chunk processed independently\n",
    "- **Returns:** List of MFCC features per chunk\n",
    "\n",
    "### `extract_features()`\n",
    "Batch feature extraction for dataset:\n",
    "- Processes multiple audio files\n",
    "- Filters to fixed MFCC length (default: 50 frames)\n",
    "- **Returns:** Features (x), labels (y), and speaker IDs\n",
    "- **Input:** List of tuples: `(folder, filename, speaker_id, label)`\n",
    "\n",
    "**Purpose:** Standardizes audio preprocessing pipeline for consistent model input across all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eb9a3d7-5ea2-473a-b0fe-965447db980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_mfcc(signal, sample_rate=8000, num_mfcc=13):\n",
    "    \"\"\"Convert audio signal to MFCC features.\"\"\"\n",
    "    mfccs = python_speech_features.base.mfcc(\n",
    "        signal,\n",
    "        samplerate=sample_rate,\n",
    "        winlen=0.025,\n",
    "        winstep=0.020,\n",
    "        numcep=num_mfcc,\n",
    "        nfilt=20,\n",
    "        nfft=256,\n",
    "        lowfreq=300,\n",
    "        highfreq=4000,\n",
    "        preemph=0.0,\n",
    "        ceplifter=0,\n",
    "        appendEnergy=True,\n",
    "        winfunc=np.hanning\n",
    "    )\n",
    "    return mfccs.transpose()\n",
    "\n",
    "\n",
    "def calc_mfcc(audio_path, sample_rate=8000, num_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from audio file.\"\"\"\n",
    "    signal, _ = librosa.load(audio_path, sr=sample_rate)\n",
    "    return signal_to_mfcc(signal, sample_rate, num_mfcc)\n",
    "\n",
    "\n",
    "def chunk_audio_signal(signal, sample_rate=8000, chunk_duration=1.0):\n",
    "    \"\"\"\n",
    "    Chunk audio signal into fixed-duration segments.\n",
    "    Returns list of MFCC features for each chunk.\n",
    "    \"\"\"\n",
    "    chunk_length = int(sample_rate * chunk_duration)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(signal) - chunk_length + 1, chunk_length):\n",
    "        chunk = signal[i:i + chunk_length]\n",
    "        mfccs = signal_to_mfcc(chunk, sample_rate)\n",
    "        chunks.append(mfccs)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_features(dataset_path, file_data, len_mfcc=50):\n",
    "    \"\"\"\n",
    "    Extract features for list of files with speaker and label info.\n",
    "    file_data: list of (target_folder, filename, speaker_id, label)\n",
    "    Returns: x, y, speaker_ids\n",
    "    \"\"\"\n",
    "    x, y, speaker_ids = [], [], []\n",
    "    for target_folder, filename, speaker_id, label in file_data:\n",
    "        path = join(dataset_path, target_folder, filename)\n",
    "        mfccs = calc_mfcc(path)\n",
    "        if mfccs.shape[1] == len_mfcc:\n",
    "            x.append(mfccs)\n",
    "            y.append(label)\n",
    "            speaker_ids.append(speaker_id)\n",
    "    return x, y, speaker_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2aca30-a3a9-4df0-a942-e5d98bca02fd",
   "metadata": {},
   "source": [
    "## Preprocess Background Noise Files\n",
    "\n",
    "Special preprocessing for background noise audio files to handle their variable length.\n",
    "\n",
    "**Problem:** Background noise files are much longer than command utterances, causing class imbalance.\n",
    "\n",
    "**Solution:** \n",
    "1. Chunk each background noise file into 1-second segments\n",
    "2. Treat each chunk as an independent sample\n",
    "3. Assign unique virtual speaker ID to each chunk (e.g., `bg_chunk_0`, `bg_chunk_1`)\n",
    "\n",
    "**Process:**\n",
    "- Loads all files from `_background_noise_` folder\n",
    "- Splits each file into fixed-duration chunks\n",
    "- Filters chunks to match required MFCC length (50 frames)\n",
    "- Creates synthetic samples with virtual speaker IDs\n",
    "\n",
    "**Returns:** List of tuples: `(target, virtual_speaker_id, mfccs, label)`\n",
    "\n",
    "**Why this matters:**\n",
    "- Increases background noise samples for balanced training\n",
    "- Prevents overfitting on limited background noise files\n",
    "- Each chunk represents unique acoustic context\n",
    "\n",
    "**Output:** Prints number of chunks created from original background noise files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "682e323c-67e1-4e74-af5a-9695dd5b207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_background_noise(dataset_path, data_by_speaker_target, bg_label_index, len_mfcc=50):\n",
    "    \"\"\"\n",
    "    Chunk all background noise files upfront and create virtual samples.\n",
    "    Each chunk is treated as an independent sample with unique ID.\n",
    "    Returns: List of (virtual_speaker_id, mfccs, label) for each chunk\n",
    "    \"\"\"\n",
    "    bg_keys = [(target, spk) for target, spk in data_by_speaker_target.keys() \n",
    "               if target == '_background_noise_']\n",
    "    \n",
    "    if not bg_keys:\n",
    "        return []\n",
    "    \n",
    "    bg_chunks = []\n",
    "    chunk_counter = 0\n",
    "    \n",
    "    print(f\"Preprocessing {len(bg_keys)} background noise files...\")\n",
    "    \n",
    "    for target, speaker_id in bg_keys:\n",
    "        for filename, label, _ in data_by_speaker_target[(target, speaker_id)]:\n",
    "            path = join(dataset_path, target, filename)\n",
    "            signal, _ = librosa.load(path, sr=8000)\n",
    "            \n",
    "            mfcc_chunks = chunk_audio_signal(signal)\n",
    "            \n",
    "            for mfccs in mfcc_chunks:\n",
    "                if mfccs.shape[1] == len_mfcc:\n",
    "                    virtual_speaker_id = f\"bg_chunk_{chunk_counter}\"\n",
    "                    bg_chunks.append((target, virtual_speaker_id, mfccs, label))\n",
    "                    chunk_counter += 1\n",
    "    \n",
    "    print(f\"Created {len(bg_chunks)} background noise chunks from {len(bg_keys)} files\")\n",
    "    return bg_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b12cdc9-b7ec-4be4-a960-538f762cd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speech_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Load speech command data organized by (target, speaker_id).\n",
    "    Returns: target_list, data_by_speaker_target, bg_label_index\n",
    "    \"\"\"\n",
    "    target_list = [name for name in listdir(dataset_path) \n",
    "                   if isdir(join(dataset_path, name)) and name != '.ipynb_checkpoints']\n",
    "    \n",
    "    data_by_speaker_target = defaultdict(list)\n",
    "    bg_label_index = None\n",
    "    \n",
    "    for target_idx, target in enumerate(target_list):\n",
    "        files = [f for f in listdir(join(dataset_path, target)) if f.endswith('.wav')]\n",
    "        \n",
    "        is_background = (target == '_background_noise_')\n",
    "        if is_background:\n",
    "            bg_label_index = target_idx\n",
    "        \n",
    "        for file in files:\n",
    "            speaker_id = file.split('_')[0]\n",
    "            data_by_speaker_target[(target, speaker_id)].append((file, target_idx, is_background))\n",
    "    \n",
    "    return target_list, data_by_speaker_target, bg_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bad12990-261a-4947-b03a-d5e1ed9ed440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(data_by_speaker_target, bg_chunks, n_splits=10, val_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Create K-fold splits respecting speaker IDs.\n",
    "    Background noise chunks are treated as individual speakers.\n",
    "    \"\"\"\n",
    "    non_bg_keys = [(target, spk) for target, spk in data_by_speaker_target.keys() \n",
    "                   if target != '_background_noise_']\n",
    "    \n",
    "    bg_speaker_keys = [(chunk[0], chunk[1]) for chunk in bg_chunks] \n",
    "    \n",
    "    all_speaker_keys = non_bg_keys + bg_speaker_keys\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_speaker_keys)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = []\n",
    "    \n",
    "    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(all_speaker_keys)):\n",
    "        train_val_keys = [all_speaker_keys[i] for i in train_val_idx]\n",
    "        test_keys = [all_speaker_keys[i] for i in test_idx]\n",
    "        \n",
    "        val_size = int(len(train_val_keys) * val_ratio)\n",
    "        val_keys = train_val_keys[:val_size]\n",
    "        train_keys = train_val_keys[val_size:]\n",
    "        \n",
    "        folds.append((train_keys, val_keys, test_keys))\n",
    "        print(f\"Fold {fold_idx + 1}/{n_splits} created\")\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "383da114-c623-4725-9d60-3490141a9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_partition(dataset_path, data_by_speaker_target, bg_chunks_dict, speaker_keys, len_mfcc=50):\n",
    "    \"\"\"\n",
    "    Extract features for a given partition with speaker ID tracking.\n",
    "    Handles both regular files and pre-chunked background noise.\n",
    "    Returns: x, y, speaker_ids\n",
    "    \"\"\"\n",
    "    x, y, speaker_ids = [], [], []\n",
    "    \n",
    "    for target, speaker_id in speaker_keys:\n",
    "        if speaker_id in bg_chunks_dict:\n",
    "            mfccs, label = bg_chunks_dict[speaker_id]\n",
    "            x.append(mfccs)\n",
    "            y.append(label)\n",
    "            speaker_ids.append(speaker_id)\n",
    "        else:\n",
    "            for filename, label, is_background in data_by_speaker_target[(target, speaker_id)]:\n",
    "                if not is_background:  \n",
    "                    path = join(dataset_path, target, filename)\n",
    "                    mfccs = calc_mfcc(path)\n",
    "                    if mfccs.shape[1] == len_mfcc:\n",
    "                        x.append(mfccs)\n",
    "                        y.append(label)\n",
    "                        speaker_ids.append(speaker_id)\n",
    "    \n",
    "    return x, y, speaker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0507733-3689-46ba-9555-a3a33cf00bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_finetuning_data(x_test, y_test, speaker_ids_test, bg_label_index, ratio=0.3, seed=42):\n",
    "    \"\"\"\n",
    "    Extract 30% of test data for fine-tuning.\n",
    "    Groups by (speaker_id, label) to ensure same speaker appears in both sets.\n",
    "    Background noise chunks are EXCLUDED from fine-tuning.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    bg_indices = []\n",
    "    regular_indices = []\n",
    "    \n",
    "    for i, (y, speaker) in enumerate(zip(y_test, speaker_ids_test)):\n",
    "        if y == bg_label_index or speaker.startswith('bg_chunk_'):\n",
    "            bg_indices.append(i)\n",
    "        else:\n",
    "            regular_indices.append(i)\n",
    "\n",
    "    speaker_label_data = defaultdict(list)\n",
    "    for idx in regular_indices:\n",
    "        speaker = speaker_ids_test[idx]\n",
    "        label = y_test[idx]\n",
    "        speaker_label_data[(speaker, label)].append(idx)\n",
    "    \n",
    "    finetune_indices = set()\n",
    "\n",
    "    for (speaker, label), indices in speaker_label_data.items():\n",
    "        if len(indices) >= 2:\n",
    "            num_to_take = max(1, int(len(indices) * ratio))\n",
    "            selected = random.sample(indices, num_to_take)\n",
    "            finetune_indices.update(selected)\n",
    "\n",
    "    x_finetune = [x_test[i] for i in finetune_indices]\n",
    "    y_finetune = [y_test[i] for i in finetune_indices]\n",
    "    speaker_finetune = [speaker_ids_test[i] for i in finetune_indices]\n",
    "\n",
    "    final_test_indices = (set(regular_indices) - finetune_indices) | set(bg_indices)\n",
    "    \n",
    "    x_remaining_test = [x_test[i] for i in final_test_indices]\n",
    "    y_remaining_test = [y_test[i] for i in final_test_indices]\n",
    "    speaker_remaining = [speaker_ids_test[i] for i in final_test_indices]\n",
    "    \n",
    "    return x_finetune, y_finetune, speaker_finetune, x_remaining_test, y_remaining_test, speaker_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a22a3862-f0e5-4561-b22d-cb2f049fcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fold(fold_idx, x_train, y_train, speaker_train, x_val, y_val, speaker_val,\n",
    "              x_test, y_test, speaker_test, x_finetune, y_finetune, speaker_finetune,\n",
    "              x_final_test, y_final_test, speaker_final_test, output_dir):\n",
    "    \"\"\"Save a single fold with all partitions including speaker IDs.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    fold_file = join(output_dir, f\"fold_{fold_idx + 1}.npz\")\n",
    "    np.savez(\n",
    "        fold_file,\n",
    "        x_train_fold=x_train,\n",
    "        y_train_fold=y_train,\n",
    "        speaker_train_fold=speaker_train,\n",
    "        x_val_fold=x_val,\n",
    "        y_val_fold=y_val,\n",
    "        speaker_val_fold=speaker_val,\n",
    "        x_test_fold=x_test,\n",
    "        y_test_fold=y_test,\n",
    "        speaker_test_fold=speaker_test,\n",
    "        x_finetune_train=x_finetune,\n",
    "        y_finetune_train=y_finetune,\n",
    "        speaker_finetune_train=speaker_finetune,\n",
    "        x_final_test=x_final_test,\n",
    "        y_final_test=y_final_test,\n",
    "        speaker_final_test=speaker_final_test\n",
    "    )\n",
    "    print(f\"Fold {fold_idx + 1} saved to {fold_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b059763-31ad-448f-8eac-f9973f1f7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_folds(dataset_path, output_dir, n_splits=10, val_ratio=0.1, \n",
    "                          finetune_ratio=0.3, seed=42):\n",
    "    \"\"\"Complete pipeline: load data, create folds, extract fine-tuning data, and save.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    target_list, data_by_speaker_target, bg_label_index = load_speech_data(dataset_path)\n",
    "    print(f\"Loaded {len(target_list)} targets\")\n",
    "    \n",
    "    bg_chunks = preprocess_background_noise(dataset_path, data_by_speaker_target, bg_label_index)\n",
    "    \n",
    "    bg_chunks_dict = {chunk[1]: (chunk[2], chunk[3]) for chunk in bg_chunks} \n",
    "    \n",
    "    if bg_label_index is not None:\n",
    "        print(f\"Background noise label index: {bg_label_index}\")\n",
    "    else:\n",
    "        print(\"Warning: No background noise found\")\n",
    "    \n",
    "    print(f\"\\nCreating {n_splits} folds...\")\n",
    "    folds = create_k_folds(data_by_speaker_target, bg_chunks, n_splits, val_ratio, seed)\n",
    "    \n",
    "    print(\"\\nExtracting features and creating partitions...\")\n",
    "    for fold_idx, (train_keys, val_keys, test_keys) in enumerate(folds):\n",
    "        print(f\"\\nProcessing Fold {fold_idx + 1}/{n_splits}\")\n",
    "        \n",
    "        x_train, y_train, speaker_train = extract_partition(dataset_path, data_by_speaker_target, bg_chunks_dict, train_keys)\n",
    "        x_val, y_val, speaker_val = extract_partition(dataset_path, data_by_speaker_target, bg_chunks_dict, val_keys)\n",
    "        x_test, y_test, speaker_test = extract_partition(dataset_path, data_by_speaker_target, bg_chunks_dict, test_keys)\n",
    "        \n",
    "        x_finetune, y_finetune, speaker_finetune, x_final_test, y_final_test, speaker_final_test = create_finetuning_data(\n",
    "            x_test, y_test, speaker_test, bg_label_index, finetune_ratio, seed + fold_idx\n",
    "        )\n",
    "        \n",
    "        print(f\"  Train: {len(x_train)}, Val: {len(x_val)}, Test: {len(x_test)}\")\n",
    "        print(f\"  Fine-tune: {len(x_finetune)}, Final Test: {len(x_final_test)}\")\n",
    "        \n",
    "        save_fold(fold_idx, x_train, y_train, speaker_train, x_val, y_val, speaker_val,\n",
    "                  x_test, y_test, speaker_test, x_finetune, y_finetune, speaker_finetune,\n",
    "                  x_final_test, y_final_test, speaker_final_test, output_dir)\n",
    "    \n",
    "    print(f\"\\nAll folds saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d04b32-da64-42e1-bb33-94119e3a89e5",
   "metadata": {},
   "source": [
    "## Create and Save K-Fold Data Partitions\n",
    "\n",
    "Generate 10-fold cross-validation splits with speaker-independent partitioning and save to disk.\n",
    "\n",
    "**Dataset Configuration:**\n",
    "- **Dataset path:** Google Speech Commands v0.02\n",
    "- **Number of folds:** 10\n",
    "- **Validation split:** 10% of training data per fold\n",
    "- **Fine-tuning split:** 30% of training data reserved for fine-tuning\n",
    "- **Random seed:** 42 (for reproducibility)\n",
    "\n",
    "**Splitting Strategy:**\n",
    "- **Speaker-independent:** No speaker appears in both train and test sets\n",
    "- **Stratified:** Maintains class distribution across folds\n",
    "- **Special handling:** Background noise chunks treated as separate samples\n",
    "\n",
    "**Output Files:** \n",
    "Creates `fold_1.npz` through `fold_10.npz` in `new_Data_particions/` directory.\n",
    "\n",
    "**Each .npz file contains:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e32fbf0c-3ebd-470c-ac35-bf54bbb2202d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 36 targets\n",
      "Preprocessing 6 background noise files...\n",
      "Created 398 background noise chunks from 6 files\n",
      "Background noise label index: 5\n",
      "\n",
      "Creating 10 folds...\n",
      "Fold 1/10 created\n",
      "Fold 2/10 created\n",
      "Fold 3/10 created\n",
      "Fold 4/10 created\n",
      "Fold 5/10 created\n",
      "Fold 6/10 created\n",
      "Fold 7/10 created\n",
      "Fold 8/10 created\n",
      "Fold 9/10 created\n",
      "Fold 10/10 created\n",
      "\n",
      "Extracting features and creating partitions...\n",
      "\n",
      "Processing Fold 1/10\n",
      "  Train: 77912, Val: 8253, Test: 9637\n",
      "  Fine-tune: 2116, Final Test: 7521\n",
      "Fold 1 saved to new_Data_particions/fold_1.npz\n",
      "\n",
      "Processing Fold 2/10\n",
      "  Train: 77821, Val: 8256, Test: 9725\n",
      "  Fine-tune: 2201, Final Test: 7524\n",
      "Fold 2 saved to new_Data_particions/fold_2.npz\n",
      "\n",
      "Processing Fold 3/10\n",
      "  Train: 77973, Val: 8263, Test: 9566\n",
      "  Fine-tune: 2161, Final Test: 7405\n",
      "Fold 3 saved to new_Data_particions/fold_3.npz\n",
      "\n",
      "Processing Fold 4/10\n",
      "  Train: 77820, Val: 8281, Test: 9701\n",
      "  Fine-tune: 2151, Final Test: 7550\n",
      "Fold 4 saved to new_Data_particions/fold_4.npz\n",
      "\n",
      "Processing Fold 5/10\n",
      "  Train: 78003, Val: 8252, Test: 9547\n",
      "  Fine-tune: 2138, Final Test: 7409\n",
      "Fold 5 saved to new_Data_particions/fold_5.npz\n",
      "\n",
      "Processing Fold 6/10\n",
      "  Train: 77987, Val: 8298, Test: 9517\n",
      "  Fine-tune: 2146, Final Test: 7371\n",
      "Fold 6 saved to new_Data_particions/fold_6.npz\n",
      "\n",
      "Processing Fold 7/10\n",
      "  Train: 78139, Val: 8252, Test: 9411\n",
      "  Fine-tune: 2102, Final Test: 7309\n",
      "Fold 7 saved to new_Data_particions/fold_7.npz\n",
      "\n",
      "Processing Fold 8/10\n",
      "  Train: 77895, Val: 8304, Test: 9603\n",
      "  Fine-tune: 2171, Final Test: 7432\n",
      "Fold 8 saved to new_Data_particions/fold_8.npz\n",
      "\n",
      "Processing Fold 9/10\n",
      "  Train: 77896, Val: 8304, Test: 9602\n",
      "  Fine-tune: 2156, Final Test: 7446\n",
      "Fold 9 saved to new_Data_particions/fold_9.npz\n",
      "\n",
      "Processing Fold 10/10\n",
      "  Train: 77991, Val: 8318, Test: 9493\n",
      "  Fine-tune: 2104, Final Test: 7389\n",
      "Fold 10 saved to new_Data_particions/fold_10.npz\n",
      "\n",
      "All folds saved to new_Data_particions\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"path/to/speech_commands_v0.02\"\n",
    "output_dir = \"new_Data_particions\"\n",
    "\n",
    "create_and_save_folds(\n",
    "    dataset_path=dataset_path,\n",
    "    output_dir=output_dir,\n",
    "    n_splits=10,\n",
    "    val_ratio=0.1,\n",
    "    finetune_ratio=0.3,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff886915-c851-4159-851b-f594e07c0a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays inside the .npz file:\n",
      "['x_train_fold', 'y_train_fold', 'speaker_train_fold', 'x_val_fold', 'y_val_fold', 'speaker_val_fold', 'x_test_fold', 'y_test_fold', 'speaker_test_fold', 'x_finetune_train', 'y_finetune_train', 'speaker_finetune_train', 'x_final_test', 'y_final_test', 'speaker_final_test']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('new_Data_particions/fold_1.npz')  \n",
    "\n",
    "print(\"Arrays inside the .npz file:\")\n",
    "print(data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc62ff14-08b4-4c5d-8717-d01695f17ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fold(fold_path):\n",
    "    \"\"\"Load a single fold file.\"\"\"\n",
    "    return np.load(fold_path, allow_pickle=True)\n",
    "\n",
    "\n",
    "def analyze_fold_distribution(fold_data, target_list=None):\n",
    "    \"\"\"\n",
    "    Analyze and print distribution of targets across all partitions.\n",
    "    \n",
    "    Args:\n",
    "        fold_data: Loaded .npz file\n",
    "        target_list: Optional list of target names for better readability\n",
    "    \"\"\"\n",
    "    partitions = {\n",
    "        'Training': ('y_train_fold', 'speaker_train_fold'),\n",
    "        'Validation': ('y_val_fold', 'speaker_val_fold'),\n",
    "        'Test (Full)': ('y_test_fold', 'speaker_test_fold'),\n",
    "        'Fine-tune Train': ('y_finetune_train', 'speaker_finetune_train'),\n",
    "        'Final Test': ('y_final_test', 'speaker_final_test')\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FOLD DATA DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for partition_name, (y_key, speaker_key) in partitions.items():\n",
    "        y_data = fold_data[y_key]\n",
    "        speaker_data = fold_data[speaker_key]\n",
    "        \n",
    "        print(f\"\\n{partition_name.upper()}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Total samples: {len(y_data)}\")\n",
    "        print(f\"Unique speakers: {len(set(speaker_data))}\")\n",
    "        \n",
    "        # Count by target\n",
    "        target_counts = Counter(y_data)\n",
    "        print(f\"\\nDistribution by target (label):\")\n",
    "        for target_idx in sorted(target_counts.keys()):\n",
    "            target_name = target_list[target_idx] if target_list and target_idx < len(target_list) else f\"Label_{target_idx}\"\n",
    "            count = target_counts[target_idx]\n",
    "            print(f\"  {target_name:30s}: {count:5d} samples\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6dd001a-9717-4975-9da2-5c1a13a21e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_partitions(fold_data):\n",
    "    \"\"\"Compare original test vs fine-tune + final test.\"\"\"\n",
    "    y_test_full = fold_data['y_test_fold']\n",
    "    y_finetune = fold_data['y_finetune_train']\n",
    "    y_final_test = fold_data['y_final_test']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PARTITION SPLIT VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Original Test Set:     {len(y_test_full)} samples\")\n",
    "    print(f\"Fine-tune Train:       {len(y_finetune)} samples ({len(y_finetune)/len(y_test_full)*100:.1f}%)\")\n",
    "    print(f\"Final Test:            {len(y_final_test)} samples ({len(y_final_test)/len(y_test_full)*100:.1f}%)\")\n",
    "    print(f\"Sum (Finetune + Final): {len(y_finetune) + len(y_final_test)} samples\")\n",
    "    \n",
    "    if len(y_finetune) + len(y_final_test) == len(y_test_full):\n",
    "        print(\"Split is correct - no data loss\")\n",
    "    else:\n",
    "        print(\"WARNING: Data mismatch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1bc30b1-e067-4fcb-8685-69c7d5e79f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_speaker_overlap(fold_data):\n",
    "    \"\"\"Check if same speakers appear in fine-tune and final test (should be YES).\"\"\"\n",
    "    speaker_finetune = set(fold_data['speaker_finetune_train'])\n",
    "    speaker_final_test = set(fold_data['speaker_final_test'])\n",
    "    \n",
    "    overlap = speaker_finetune & speaker_final_test\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SPEAKER OVERLAP ANALYSIS (Fine-tune ↔ Final Test)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Speakers in Fine-tune Train: {len(speaker_finetune)}\")\n",
    "    print(f\"Speakers in Final Test:      {len(speaker_final_test)}\")\n",
    "    print(f\"Overlapping speakers:        {len(overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43cdc9d8-fafc-45cb-a08f-c71ae501055d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FOLD DATA DISTRIBUTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "TRAINING\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples: 77912\n",
      "Unique speakers: 2765\n",
      "\n",
      "Distribution by target (label):\n",
      "  stop                          :  2916 samples\n",
      "  up                            :  2614 samples\n",
      "  learn                         :  1152 samples\n",
      "  bird                          :  1449 samples\n",
      "  follow                        :  1219 samples\n",
      "  _background_noise_            :   314 samples\n",
      "  wow                           :  1449 samples\n",
      "  on                            :  2834 samples\n",
      "  marvin                        :  1454 samples\n",
      "  tree                          :  1162 samples\n",
      "  no                            :  2911 samples\n",
      "  dog                           :  1473 samples\n",
      "  happy                         :  1415 samples\n",
      "  off                           :  2799 samples\n",
      "  down                          :  2937 samples\n",
      "  six                           :  2895 samples\n",
      "  sheila                        :  1389 samples\n",
      "  bed                           :  1359 samples\n",
      "  seven                         :  2975 samples\n",
      "  visual                        :  1215 samples\n",
      "  four                          :  2791 samples\n",
      "  right                         :  2826 samples\n",
      "  five                          :  2994 samples\n",
      "  cat                           :  1413 samples\n",
      "  house                         :  1483 samples\n",
      "  left                          :  2892 samples\n",
      "  go                            :  2848 samples\n",
      "  eight                         :  2785 samples\n",
      "  forward                       :  1161 samples\n",
      "  one                           :  2878 samples\n",
      "  yes                           :  3038 samples\n",
      "  two                           :  2869 samples\n",
      "  backward                      :  1252 samples\n",
      "  nine                          :  2998 samples\n",
      "  three                         :  2715 samples\n",
      "  zero                          :  3038 samples\n",
      "\n",
      "VALIDATION\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples: 8253\n",
      "Unique speakers: 1838\n",
      "\n",
      "Distribution by target (label):\n",
      "  stop                          :   250 samples\n",
      "  up                            :   294 samples\n",
      "  learn                         :   125 samples\n",
      "  bird                          :   162 samples\n",
      "  follow                        :   111 samples\n",
      "  _background_noise_            :    31 samples\n",
      "  wow                           :   156 samples\n",
      "  on                            :   297 samples\n",
      "  marvin                        :   172 samples\n",
      "  tree                          :   135 samples\n",
      "  no                            :   313 samples\n",
      "  dog                           :   167 samples\n",
      "  happy                         :   173 samples\n",
      "  off                           :   304 samples\n",
      "  down                          :   289 samples\n",
      "  six                           :   299 samples\n",
      "  sheila                        :   178 samples\n",
      "  bed                           :   161 samples\n",
      "  seven                         :   317 samples\n",
      "  visual                        :    92 samples\n",
      "  four                          :   250 samples\n",
      "  right                         :   296 samples\n",
      "  five                          :   352 samples\n",
      "  cat                           :   156 samples\n",
      "  house                         :   167 samples\n",
      "  left                          :   289 samples\n",
      "  go                            :   325 samples\n",
      "  eight                         :   281 samples\n",
      "  forward                       :   138 samples\n",
      "  one                           :   304 samples\n",
      "  yes                           :   306 samples\n",
      "  two                           :   279 samples\n",
      "  backward                      :   114 samples\n",
      "  nine                          :   305 samples\n",
      "  three                         :   330 samples\n",
      "  zero                          :   335 samples\n",
      "\n",
      "TEST (FULL)\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples: 9637\n",
      "Unique speakers: 1952\n",
      "\n",
      "Distribution by target (label):\n",
      "  stop                          :   397 samples\n",
      "  up                            :   361 samples\n",
      "  learn                         :   149 samples\n",
      "  bird                          :   147 samples\n",
      "  follow                        :   124 samples\n",
      "  _background_noise_            :    53 samples\n",
      "  wow                           :   192 samples\n",
      "  on                            :   341 samples\n",
      "  marvin                        :   206 samples\n",
      "  tree                          :   177 samples\n",
      "  no                            :   321 samples\n",
      "  dog                           :   181 samples\n",
      "  happy                         :   178 samples\n",
      "  off                           :   324 samples\n",
      "  down                          :   354 samples\n",
      "  six                           :   404 samples\n",
      "  sheila                        :   187 samples\n",
      "  bed                           :   167 samples\n",
      "  seven                         :   377 samples\n",
      "  visual                        :   162 samples\n",
      "  four                          :   362 samples\n",
      "  right                         :   326 samples\n",
      "  five                          :   381 samples\n",
      "  cat                           :   148 samples\n",
      "  house                         :   181 samples\n",
      "  left                          :   321 samples\n",
      "  go                            :   305 samples\n",
      "  eight                         :   363 samples\n",
      "  forward                       :   153 samples\n",
      "  one                           :   310 samples\n",
      "  yes                           :   348 samples\n",
      "  two                           :   377 samples\n",
      "  backward                      :   192 samples\n",
      "  nine                          :   326 samples\n",
      "  three                         :   358 samples\n",
      "  zero                          :   384 samples\n",
      "\n",
      "FINE-TUNE TRAIN\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples: 2116\n",
      "Unique speakers: 1148\n",
      "\n",
      "Distribution by target (label):\n",
      "  stop                          :    83 samples\n",
      "  up                            :    81 samples\n",
      "  learn                         :    33 samples\n",
      "  bird                          :    25 samples\n",
      "  follow                        :    28 samples\n",
      "  wow                           :    40 samples\n",
      "  on                            :    80 samples\n",
      "  marvin                        :    53 samples\n",
      "  tree                          :    33 samples\n",
      "  no                            :    71 samples\n",
      "  dog                           :    34 samples\n",
      "  happy                         :    33 samples\n",
      "  off                           :    71 samples\n",
      "  down                          :    83 samples\n",
      "  six                           :    90 samples\n",
      "  sheila                        :    38 samples\n",
      "  bed                           :    29 samples\n",
      "  seven                         :    89 samples\n",
      "  visual                        :    36 samples\n",
      "  four                          :    89 samples\n",
      "  right                         :    81 samples\n",
      "  five                          :    81 samples\n",
      "  cat                           :    23 samples\n",
      "  house                         :    30 samples\n",
      "  left                          :    70 samples\n",
      "  go                            :    72 samples\n",
      "  eight                         :    86 samples\n",
      "  forward                       :    35 samples\n",
      "  one                           :    72 samples\n",
      "  yes                           :    77 samples\n",
      "  two                           :    84 samples\n",
      "  backward                      :    45 samples\n",
      "  nine                          :    74 samples\n",
      "  three                         :    80 samples\n",
      "  zero                          :    87 samples\n",
      "\n",
      "FINAL TEST\n",
      "--------------------------------------------------------------------------------\n",
      "Total samples: 7521\n",
      "Unique speakers: 1952\n",
      "\n",
      "Distribution by target (label):\n",
      "  stop                          :   314 samples\n",
      "  up                            :   280 samples\n",
      "  learn                         :   116 samples\n",
      "  bird                          :   122 samples\n",
      "  follow                        :    96 samples\n",
      "  _background_noise_            :    53 samples\n",
      "  wow                           :   152 samples\n",
      "  on                            :   261 samples\n",
      "  marvin                        :   153 samples\n",
      "  tree                          :   144 samples\n",
      "  no                            :   250 samples\n",
      "  dog                           :   147 samples\n",
      "  happy                         :   145 samples\n",
      "  off                           :   253 samples\n",
      "  down                          :   271 samples\n",
      "  six                           :   314 samples\n",
      "  sheila                        :   149 samples\n",
      "  bed                           :   138 samples\n",
      "  seven                         :   288 samples\n",
      "  visual                        :   126 samples\n",
      "  four                          :   273 samples\n",
      "  right                         :   245 samples\n",
      "  five                          :   300 samples\n",
      "  cat                           :   125 samples\n",
      "  house                         :   151 samples\n",
      "  left                          :   251 samples\n",
      "  go                            :   233 samples\n",
      "  eight                         :   277 samples\n",
      "  forward                       :   118 samples\n",
      "  one                           :   238 samples\n",
      "  yes                           :   271 samples\n",
      "  two                           :   293 samples\n",
      "  backward                      :   147 samples\n",
      "  nine                          :   252 samples\n",
      "  three                         :   278 samples\n",
      "  zero                          :   297 samples\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PARTITION SPLIT VERIFICATION\n",
      "================================================================================\n",
      "Original Test Set:     9637 samples\n",
      "Fine-tune Train:       2116 samples (22.0%)\n",
      "Final Test:            7521 samples (78.0%)\n",
      "Sum (Finetune + Final): 9637 samples\n",
      "Split is correct - no data loss\n",
      "\n",
      "================================================================================\n",
      "SPEAKER OVERLAP ANALYSIS (Fine-tune ↔ Final Test)\n",
      "================================================================================\n",
      "Speakers in Fine-tune Train: 1148\n",
      "Speakers in Final Test:      1952\n",
      "Overlapping speakers:        1148\n"
     ]
    }
   ],
   "source": [
    "fold_path = \"new_Data_particions/fold_1.npz\"\n",
    "fold_data = load_fold(fold_path)\n",
    "\n",
    "target_list = ['stop', 'up', 'learn', 'bird', 'follow', '_background_noise_', 'wow', 'on', 'marvin', 'tree', 'no', 'dog', 'happy', 'off', 'down', 'six', 'sheila', 'bed', 'seven', 'visual', 'four', 'right', 'five', 'cat', 'house', 'left', 'go', 'eight', 'forward', 'one', 'yes', 'two', 'backward', 'nine', 'three', 'zero'] \n",
    "\n",
    "analyze_fold_distribution(fold_data, target_list)\n",
    "compare_partitions(fold_data)\n",
    "check_speaker_overlap(fold_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
